{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Junior Data Science Klub - 2025-ös verseny\n",
    "Tóth Zoltán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az adatok előkészítése a nagy mennyiségre tekintettel SQL-ben történt (az adatokat a megadott csv-ből olvastam be). \n",
    "Ennek kódja (tábla létrehozűsa és utána a releváns események lekérése - azon userek összes esménye, ahol volt 'trial_started_event'): \n",
    "\n",
    "CREATE TABLE telepromter_fixed (\n",
    "    event VARCHAR(255) NOT NULL,        \n",
    "    event_time BIGINT NOT NULL,          \n",
    "    distinct_id VARCHAR(255) NOT NULL, \n",
    "    os_version VARCHAR(50),            \n",
    "    country_code VARCHAR(10)           \n",
    "\n",
    "BULK INSERT dbo.telepromter_fixed\n",
    "FROM 'C:\\Users\\tothz\\Documents\\2_bgf\\jds\\klubverseny_25\\teleprompter_hashed_fixed_clean.csv'\n",
    "WITH (\n",
    "    FIELDTERMINATOR = ';',  \n",
    "    ROWTERMINATOR = '\\n',   \n",
    "    FIRSTROW = 2            \n",
    ");\n",
    "\n",
    "select COUNT (event) from dbo.telepromter_fixed;\n",
    "\n",
    "SELECT *\n",
    "FROM dbo.telepromter_fixed\n",
    "WHERE (distinct_id) IN (\n",
    "    SELECT DISTINCT (distinct_id) as 'user'\n",
    "    FROM dbo.telepromter_fixed\n",
    "    WHERE event = 'trial_started_event'\n",
    ");\n",
    "\n",
    "SELECT *\n",
    "INTO dbo.esemenyek\n",
    "FROM dbo.telepromter_fixed\n",
    "WHERE distinct_id IN (\n",
    "    SELECT DISTINCT distinct_id\n",
    "    FROM dbo.telepromter_fixed\n",
    "    WHERE event = 'trial_started_event'\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A további feldolgozás, elemzés Pythonban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "base_dir = r'C:\\Users\\tothz\\Documents\\2_bgf\\jds\\klubverseny_25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_user = pd.read_csv(r'C:\\Users\\tothz\\Documents\\2_bgf\\jds\\klubverseny_25\\trialosok_esemenyei_fixed.csv', sep=';', header=None, names=['event', 'event_time', 'distinct_id', 'os_version', 'country_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_user['event_time_new'] = pd.to_datetime(df_trial_user['event_time'], unit='s')\n",
    "#uj oszlop a most trailt kezdő userek közül konvertáltra\n",
    "converted_users = df_trial_user[df_trial_user['event'].isin(['trial_converted_event', 'initial_purchase_event'])]['distinct_id'].unique()\n",
    "df_trial_user['converted'] = df_trial_user['distinct_id'].isin(converted_users).astype(int)\n",
    "\n",
    "df_trial_user.to_csv('teleprompter_1_fixed.csv', index=False)\n",
    "\n",
    "#az egyes események száma \n",
    "esemenyek_csop = df_trial_user.groupby('event').count().reset_index()\n",
    "esemenyek_csop.to_excel('esem_csop_fixed.xlsx', index=False)\n",
    "\n",
    "#események szama converted-nem converted bontasban\n",
    "esem_csop = df_trial_user.groupby(['event', 'converted']).size().reset_index(name='event_count')\n",
    "esem_csop = esem_csop.pivot(index='event', columns='converted', values='event_count').fillna(0).astype(int)\n",
    "esem_csop.columns = ['converted_0', 'converted_1']\n",
    "esem_csop['sum'] = esem_csop['converted_0'] + esem_csop['converted_1']\n",
    "esem_csop.to_csv('esem_csop.csv', index=False)\n",
    "\n",
    "#szűkitünk arra, ahol a trail már befejeződött (7 nap)\n",
    "condition_1 = (df_trial_user['event'] == 'trial_start_event') & (df_trial_user['event_time_new'] < '2025-02-22')\n",
    "condition_2 = df_trial_user['event'].isin(['trial_converted_event', 'trial_cancelled_event', 'initial_purchase_event'])\n",
    "\n",
    "# Azoknak a user_id-knek a kiválasztása, akiknél legalább az egyik feltétel teljesül\n",
    "target_user_ids = df_trial_user[condition_1 | condition_2]['distinct_id'].unique()\n",
    "\n",
    "# Szűrés azokra a sorokra, ahol a user_id szerepel a target_user_ids között - ez lesz az elemzo adatbazisunk\n",
    "df_trial_2 = df_trial_user[df_trial_user['distinct_id'].isin(target_user_ids)]\n",
    "df_trial_2.to_csv('esem_szuk_fixed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESEMENYTIPUSOK SZURESE\n",
    "# csinálunk egy olyan táblát, amibe az egyes eseménytipusok vannak osszeszamolva convertalt, nem convertalt bontasban "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "esem_csop = df_trial_2.groupby(['event', 'converted']).size().reset_index(name='event_count')\n",
    "esem_csop = esem_csop.pivot(index='event', columns='converted', values='event_count').fillna(0).astype(int)\n",
    "esem_csop.columns = ['converted_0', 'converted_1']\n",
    "esem_csop['sum'] = esem_csop['converted_0'] + esem_csop['converted_1']\n",
    "esem_csop.to_excel('esem_csop.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beolvassuk az esemnytipusok leirarsat es osszekotjkuk  a ket tablat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esem_def=pd.read_excel(r'C:\\Users\\tothz\\Documents\\2_bgf\\jds\\klubverseny_25\\TP_event_descriptions.xlsx')\n",
    "esem_def=esem_csop.merge(sem_def, how='left', left_on='event', right_on='event_name')\n",
    "esem_def.to_excel('esem_csop_2.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Excelben készült egy funkciókat összevonó excel (esemény - eseménycsoport/funkció) a hibákat különákülön csoportkélnt kezelve az egyes funkcióknál\n",
    "# ezt olvassukl be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esem_csop=pd.read_excel(r'C:\\Users\\tothz\\Documents\\2_bgf\\jds\\klubverseny_25\\esem_csop_3.xlsx', sheet_name='csoportok2')\n",
    "df_trial_3=pd.read_csv('esem_szuk_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kiakalitjuk az osztályozáshoz az alaptáblát. Egy sor egy user, oszlopok az egyes eseménycsoportkban a user eseményei száma\n",
    "#+ hozzárakjuk az egyéb user jellemző változokat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_4 = df_trial_3.merge(esem_csop[['event_name', 'funkcio']], how='left', left_on='event', right_on='event_name')\n",
    "\n",
    "# Felössszegzés az egyes 'distinct_id' és 'funkcio' kombinációkra\n",
    "df_trial_5 = df_trial_4.groupby(['distinct_id', 'funkcio'])['funkcio'].count().reset_index(name='count')\n",
    "\n",
    "# Pivotálás az események összesítéséhez 'distinct_id' szintjén\n",
    "df_pivot = df_trial_5.pivot(index='distinct_id', columns='funkcio', values='count').fillna(0).astype(int)\n",
    "\n",
    "# Az eredeti 'distinct_id'-nél szükséges user jellemzők (oszlopok) megőrzése\n",
    "constant_columns = ['distinct_id', 'os_version', 'country_code', 'converted']\n",
    "df_constants = df_trial_4[constant_columns].drop_duplicates(subset='distinct_id').set_index('distinct_id')\n",
    "\n",
    "# Összefűzzük a pivotált DataFrame-et és a user tulajdonságokat \n",
    "df_alap = pd.concat([df_constants, df_pivot], axis=1).reset_index()\n",
    "\n",
    "# A 'distinct_id' visszaállítása oszlopként\n",
    "df_alap = df_alap.reset_index()\n",
    "df_alap['distinct_id']  = df_alap['index']\n",
    "df_alap = df_alap.drop(columns=['level_0', 'index'])\n",
    "\n",
    "# oszlopsorrend megvaltoztatasa\n",
    "df_alap = df_alap[['distinct_id', 'os_version', 'country_code', 'converted', 'account','login', 'payment','AI Script Generator',\n",
    "       'AI Script Generator_error', 'Add Intro/Outro', 'Add Subtitles',\n",
    "       'Add Subtitles_error', 'Add Watermark', 'Apple Watch_error',\n",
    "       'Bluetooth Remote', 'Blur Background', 'Change Margins', 'Clean Audio',\n",
    "       'Clean Audio_error', 'Countdown Before Starting', 'Custom Fonts',\n",
    "       'Display Remaining Time', 'Export Raw Video', 'Export Raw Video_error',\n",
    "       'Facebook', 'Flip Camera During Recording', 'Folder',\n",
    "       'Font Size Change', 'Footpedal', 'Game Controller',\n",
    "       'Keyboard Shortcuts', 'Lexend Support', 'Mirroring',\n",
    "       'Open Dyslexic Support', 'Presentation Remote', 'Progress Bar',\n",
    "       'Referral Program', 'Registration', 'Registration_error',\n",
    "       'Replace Background', 'Replace Green Screen', 'Resize Video',\n",
    "       'Resize Video_error', 'Script_generation', 'Script_generation_error',\n",
    "       'Select Camera', 'Select FPS', 'Select Microphone',\n",
    "       'Select Microphone_error', 'Social Log In', 'Speed Based Scrolling',\n",
    "       'Stream with URL', 'Tap to Scroll', 'Trim Video', 'Visual Timer',\n",
    "       'Web-based Remote', 'YouTube', 'account_error', 'facebook',\n",
    "       'live_stream', 'live_stream_error', 'login_error',\n",
    "       'onboarding', 'onboarding_error', 'other_error', 'payment_error', 'recording', 'recording_error', 'remote',\n",
    "       'remote_error', 'select camera']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# időváltozók képzése és hozáfűzése\n",
    "\n",
    "time_features = df_trial_4.groupby('distinct_id')['event_time_new'].agg(['min', 'max']).reset_index()\n",
    "df_alap = pd.merge(df_alap, time_features, on='distinct_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DF_ALAP lesz az alaptáblánk a továbi elemzéshez, ahol funkciókat és nem eseményket használunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OSZTÁLYOZÁS - konverted a célváltozó (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# első modell: logisztikus regresszió"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "X = df_alap.drop(columns=['converted', 'distinct_id', 'os_version', 'country_code', 'min', 'max'])\n",
    "y = df_alap['converted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#kiértékelés (confuzios matrix, auroc)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Pontosság:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Jelentés:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUROC: {auroc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## második model: döntési fa, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#NaN kezelése az os versionban\n",
    "df_alap['os_version'] = df_alap['os_version'].fillna('Unknown')\n",
    "\n",
    "# countrz code és os version átkodolása\n",
    "le_country = LabelEncoder()\n",
    "df_alap['country_code_encoded'] = le_country.fit_transform(df_alap['country_code'])\n",
    "\n",
    "le_os_version = LabelEncoder()\n",
    "df_alap['os_version_encoded'] = le_os_version.fit_transform(df_alap['os_version'])\n",
    "\n",
    "\n",
    "# Az input változók kiválasztása \n",
    "X2 = df_alap.drop(columns=['distinct_id', 'converted', 'country_code','os_version', 'min', 'max'])\n",
    "y2 = df_alap['converted']\n",
    "\n",
    "# Adatok felosztása\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Modell tanitas\n",
    "rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model2.fit(X_train2, y_train2)\n",
    "\n",
    "# Modell értékelése (konfuzios matrix, AUROC)\n",
    "y_pred2 = rf_model2.predict(X_test2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test2, y_pred2))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "\n",
    "y_pred_proba2 = rf_model2.predict_proba(X_test2)[:, 1]\n",
    "auroc = roc_auc_score(y_test2, y_pred_proba2)\n",
    "print(f\"AUROC: {auroc:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model2.feature_importances_\n",
    "feature_names = X2.columns\n",
    "importance_df2 = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## harmadik modell: gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "## input és célváltozók azonosak, mint az előbb, teszt-tény szétválasztás is ugyanaz - indul a tanitás\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Modell értékelése (konfúzios matrix, AUROC)\n",
    "y_pred3 = gb_model.predict(X_test2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test2, y_pred3))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test2, y_pred3))\n",
    "\n",
    "y_pred_proba3 = gb_model.predict_proba(X_test2)[:, 1]\n",
    "auroc2 = roc_auc_score(y_test2, y_pred_proba3)\n",
    "print(f\"AUROC: {auroc:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "importances3 = gb_model.feature_importances_\n",
    "feature_names = X2.columns\n",
    "importance_df3 = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a random forest és gradient boosting  akét hazsnálható model, GB jobb\n",
    "# ezeknél a feature imnportance kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df2.to_excel('feature_importance2.xlsx', index=False)\n",
    "importance_df3.to_excel('feature_importance3.xlsx', index=False)\n",
    "df_alap.to_excel('df_alap.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC görbe megjelenítése\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_test2, y_pred_proba2)\n",
    "fpr2, tpr2, _ = roc_curve(y_test2, y_pred_proba3)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr1, tpr1, label=f\"Model 1 (AUROC = {auroc:.3f})\")\n",
    "plt.plot(fpr2, tpr2, label=f\"Model 2 (AUROC = {auroc2:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KLASZTEREZES - KMEAN algoritmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#adatok kiválasztása\n",
    "df_alap_cluster= df_alap[['converted', 'account','login', 'payment','AI Script Generator',\n",
    "       'AI Script Generator_error', 'Add Intro/Outro', 'Add Subtitles',\n",
    "       'Add Subtitles_error', 'Add Watermark', 'Apple Watch_error',\n",
    "       'Bluetooth Remote', 'Blur Background', 'Change Margins', 'Clean Audio',\n",
    "       'Clean Audio_error', 'Countdown Before Starting', 'Custom Fonts',\n",
    "       'Display Remaining Time', 'Export Raw Video', 'Export Raw Video_error',\n",
    "       'Facebook', 'Flip Camera During Recording', 'Folder',\n",
    "       'Font Size Change', 'Footpedal', 'Game Controller',\n",
    "       'Keyboard Shortcuts', 'Lexend Support', 'Mirroring',\n",
    "       'Open Dyslexic Support', 'Presentation Remote', 'Progress Bar',\n",
    "       'Referral Program', 'Registration', 'Registration_error',\n",
    "       'Replace Background', 'Replace Green Screen', 'Resize Video',\n",
    "       'Resize Video_error', 'Script_generation', 'Script_generation_error',\n",
    "       'Select Camera', 'Select FPS', 'Select Microphone',\n",
    "       'Select Microphone_error', 'Social Log In', 'Speed Based Scrolling',\n",
    "       'Stream with URL', 'Tap to Scroll', 'Trim Video', 'Visual Timer',\n",
    "       'Web-based Remote', 'YouTube', 'account_error', 'facebook',\n",
    "       'live_stream', 'live_stream_error', 'login_error',\n",
    "       'onboarding', 'onboarding_error', 'other_error', 'payment_error', 'recording', 'recording_error', 'remote',\n",
    "       'remote_error', 'select camera','country_code_encoded',\n",
    "       'os_version_encoded']]\n",
    "\n",
    "# Adatok skálázása szukseges - standard scalerrel\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_alap_cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideális klaszterszám kiválasztása (elbow modszerrel, grafikusan)\n",
    "inertia = []\n",
    "range_clusters = range(1, 30)\n",
    "for k in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Elbow görbe plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range_clusters, inertia, marker='o')\n",
    "plt.xlabel(\"Klaszterek száma\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method az optimális klaszterszámhoz\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5-os klaszeterszamra klszeterzés - Kmeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  \n",
    "kmeans.fit(df_alap_cluster)\n",
    "\n",
    "# Klaszter címkék hozzárendelése az eredeti DataFrame-hez\n",
    "df_alap['Cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klaszterekben a konvertált arany \n",
    "cluster_analysis = df_alap.groupby('Cluster').agg(total=('converted', 'size'), converted_mean=('converted', 'mean')).reset_index()\n",
    "\n",
    "print(\"Klaszterek összegzése:\")\n",
    "print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klaszterek vizualizációja PCA segítségével (2D térben)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(df_alap_cluster)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df_alap['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"K-means klaszterek vizualizációja (PCA)\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load matrix kiexportalása\n",
    "print(\"PCA komponens súlyai (load matrix):\")\n",
    "print(pca.components_)\n",
    "\n",
    "# PCA komponensek súlyainak DataFrame-be rendezése\n",
    "components_df = pd.DataFrame(pca.components_, columns=df_alap_cluster.columns, index=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "\n",
    "components_df.to_excel('pca_load_matrix.xlsx', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KLASZTEREK további elemzése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relativ gyakoriságok\n",
    "cluster_summary1 = cluster_analysis.groupby('Cluster').mean()\n",
    "\n",
    "cluster_summary2 = df_alap.groupby('Cluster').mean()\n",
    "\n",
    "cluster_summary2.to_excel('cluster_summary2.xlsx', index=False)\n",
    "cluster_summary1.to_excel('cluster_summary1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A KOVETKEZOKBEN AZ ESEMÉNYEKET ÖSSZEGEZZÜK FEL A USER SZINTJÉN, NEM AZ ESEMÉNYCSOPORTOKAT - TÖBB VÁLTOZÓ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kinduló adatvbázis: user-re grupba, de nem funkció, hanem esemény\n",
    "# Számoljuk meg az egyes 'distinct_id' és 'event_name' kombinációkat\n",
    "df_trial_esem = df_trial_4.groupby(['distinct_id', 'event_name'])['event_name'].count().reset_index(name='count')\n",
    "\n",
    "# Pivotálás az események összesítéséhez 'distinct_id' szintjén\n",
    "df_pivot_esem = df_trial_esem.pivot(index='distinct_id', columns='event_name', values='count').fillna(0).astype(int)\n",
    "\n",
    "# Az eredeti 'distinct_id'-nél a user tulajdonságok megőrzése - ez már fent ekészlt a funkcióra pivotáláskor\n",
    "# Összefűzzük a pivotált DataFrame-et és a tulajdonságokat\n",
    "df_alap_esem = pd.concat([df_constants, df_pivot_esem], axis=1).reset_index()\n",
    "\n",
    "# A 'distinct_id' visszaállítása oszlopként\n",
    "df_alap_esem = df_alap_esem.reset_index()\n",
    "df_alap_esem['distinct_id']  = df_alap_esem['index']\n",
    "df_alap_esem = df_alap_esem.drop(columns=['level_0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEM FOGUNK klaszterezni, de az osztályozást megcsináljuk - a két jobb modellel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN kezelése az os versionban\n",
    "df_alap_esem['os_version'] = df_alap_esem['os_version'].fillna('Unknown')\n",
    "\n",
    "# szukseges átkodolas\n",
    "le_country = LabelEncoder()\n",
    "df_alap_esem['country_code_encoded'] = le_country.fit_transform(df_alap_esem['country_code'])\n",
    "\n",
    "le_os_version = LabelEncoder()\n",
    "df_alap_esem['os_version_encoded'] = le_os_version.fit_transform(df_alap_esem['os_version'])\n",
    "\n",
    "\n",
    "# Az input változók kiválasztása\n",
    "Xe2 = df_alap_esem.drop(columns=['distinct_id', 'converted', 'country_code','os_version', 'billing_issue_event', \n",
    "    'cancellation_event', 'expiration_event', 'initial_purchase_event' , 'product_change_event', 'renewal_event',\n",
    "    'trial_cancelled_event' , 'trial_converted_event', 'trial_started_event' , 'uncancellation_event', 'subscriptionStatus_didChange'])  #itt meg a vconvertalasi esemenyeket is ki kell venni\n",
    "ye2 = df_alap_esem['converted']\n",
    "\n",
    "# Adatok felosztása\n",
    "\n",
    "X_train_e2, X_test_e2, y_train_e2, y_test_e2 = train_test_split(Xe2, ye2, test_size=0.3, random_state=42)\n",
    "\n",
    "# tanitas\n",
    "rf_model_e2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_e2.fit(X_train_e2, y_train_e2)\n",
    "\n",
    "# kiértékelés - konfizioos matrix, auroc \n",
    "y_pred_e2 = rf_model_e2.predict(X_test_e2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_e2, y_pred_e2))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_e2, y_pred_e2))\n",
    "\n",
    "y_pred_proba_e2 = rf_model_e2.predict_proba(X_test_e2)[:, 1]\n",
    "auroc_e2 = roc_auc_score(y_test_e2, y_pred_proba_e2)\n",
    "print(f\"AUROC: {auroc_e2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model_e2.feature_importances_\n",
    "feature_names = Xe2.columns\n",
    "importance_df_e2 = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_e2, X_test_e2, y_train_e2, y_test_e2 = train_test_split(Xe2, ye2, test_size=0.3, random_state=42)\n",
    "\n",
    "#tanitas\n",
    "gb_model_e3 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model_e3.fit(X_train_e2, y_train_e2)\n",
    "\n",
    "# kiértékelés\n",
    "y_pred_e3 = gb_model_e3.predict(X_test_e2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_e2, y_pred_e3))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_e2, y_pred_e3))\n",
    "\n",
    "y_pred_proba_e3 = gb_model_e3.predict_proba(X_test_e2)[:, 1]\n",
    "auroc_e3 = roc_auc_score(y_test_e2, y_pred_proba_e3)\n",
    "print(f\"AUROC: {auroc_e3:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "importances_e3 = gb_model_e3.feature_importances_\n",
    "feature_names = Xe2.columns\n",
    "importance_dfe3 = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_dfe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A legjobban teljesitő modell - gradient boosting - fejlesztése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negati arány csökentése súlyozással "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting létrehozása osztálysúlyokkal\n",
    "gb_model_e3FN = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit minták egyedi súlyozásával\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Példák súlyainak számítása az y_train osztályainak figyelembevételével\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_e2)\n",
    "\n",
    "# Modell betanítása\n",
    "gb_model_e3FN.fit(X_train_e2, y_train_e2, sample_weight=sample_weights)\n",
    "\n",
    "## kiértékelés\n",
    "y_pred_e3FN = gb_model_e3FN.predict(X_test_e2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_e2, y_pred_e3FN))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_e2, y_pred_e3FN))\n",
    "\n",
    "y_pred_proba_e3FN = gb_model_e3FN.predict_proba(X_test_e2)[:, 1]\n",
    "auroc_e3FN = roc_auc_score(y_test_e2, y_pred_proba_e3FN)\n",
    "print(f\"AUROC: {auroc_e3FN:.3f}\")\n",
    "\n",
    "importances_e3FN = gb_model_e3.feature_importances_\n",
    "feature_names = Xe2.columns\n",
    "importance_dfe3FN = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_dfe3FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# A korábbi teszt-train felosztást és változó kiválasztást használhatjuk \n",
    "# SMOTE alkalmazása a train adatokra\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_e2, y_train_e2)\n",
    "\n",
    "# model betanítása az uj adatokon\n",
    "gb_model_e3o = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model_e3o.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# kiértékelés\n",
    "y_pred_e3o = gb_model_e3o.predict(X_test_e2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_e2, y_pred_e3o))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_e2, y_pred_e3o))\n",
    "\n",
    "y_pred_proba_e3o = gb_model_e3o.predict_proba(X_test_e2)[:, 1]\n",
    "auroc_e3o = roc_auc_score(y_test_e2, y_pred_proba_e3o)\n",
    "print(f\"AUROC: {auroc_e3o:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SMOTE és class weight együttese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE alkalmazása a train adatokra\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_e2, y_train_e2)\n",
    "\n",
    "# sample weights ujraszamolasa\n",
    "sample_weights3 = compute_sample_weight(class_weight='balanced', y=y_train_resampled)\n",
    "\n",
    "\n",
    "# model betanítása\n",
    "gb_model_komb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model_komb.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights3)\n",
    "\n",
    "# kiértékelése\n",
    "y_pred_komb = gb_model_komb.predict(X_test_e2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_e2, y_pred_komb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_e2, y_pred_komb))\n",
    "\n",
    "y_pred_proba_komb = gb_model_komb.predict_proba(X_test_e2)[:, 1]\n",
    "auroc_komb = roc_auc_score(y_test_e2, y_pred_proba_komb)\n",
    "print(f\"AUROC: {auroc_komb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A class weight érdemben javitotta a Recall-t, kis AUROC és accuracy veszteség mellett \n",
    "# az oversampling gyengébb volt\n",
    "# a kombinált modell a javiott sólyozással zámoló modelt már nem javitotta érdemben, ezért elvetem: marad BG + sulyozas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hozzáfúzzük aze redeti  df-hez uj oszlpként a legjobb modellel kiszámitott 1 osztályba esési valósziműséget\n",
    "df_alap_esem_2 = df_alap_esem\n",
    "\n",
    "# Az oszlopokat kiválasztjuk a teljes adatbázisból (df_alap_esem), amit a modell megért.\n",
    "X_full = df_alap_esem_2[X_train_e2.columns]  # Biztosítsuk, hogy az X_full azonos struktúrájú legyen, mint a tanítókészlet.\n",
    "\n",
    "# 1-es osztály valószínűség kiszámítása a teljes adatbázisra.\n",
    "df_alap_esem_2['class_1_probability'] = gb_model_e3FN.predict_proba(X_full)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clustert még hozza kell tenni a df_alap -ból, a distinct ID-k alapján "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cel = df_alap_esem_2.merge(df_alap[['distinct_id', 'Cluster']], how='left', on='distinct_id')\n",
    "df_cel.to_excel('cel.xlsx', index= 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esem_csop = esem_csop.set_index('event_name')\n",
    "importance_dfe3FN = importance_dfe3FN.set_index('Feature').join(esem_csop[['sum', 'converted_0', 'converted_1']],how='left').reset_index()\n",
    "importance_dfe3FN.to_excel('feature_importnace_FN_gyakorisagok.xlsx', index= 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UJ ELEMZES - csak a trial period alatti események alapjan osztályozás, konvertálás prediktálása (konverzio valószinűsége kiszámitása)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alaptábla az eseményket még nem aggregáltan tartalmazó df_trial_4 tábla - ezt kell leszűrnünk a kivánt eseményekre\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. Az első 'trial start event' időpontjának azonosítása userenként\n",
    "df_trial_4['event_time_new'] = pd.to_datetime(df_trial_4['event_time_new'])  # Ha még nem datetime\n",
    "first_trial_times = df_trial_4[df_trial_4['event'] == 'trial_started_event'] \\\n",
    "    .groupby('distinct_id')['event_time_new'].min().reset_index()\n",
    "first_trial_times.rename(columns={'event_time_new': 'first_trial_time'}, inplace=True)\n",
    "\n",
    "df_trial_4 = df_trial_4.drop(columns=['first_trial_time'], errors='ignore')\n",
    "df_trial_szuk = df_trial_4.merge(first_trial_times, on='distinct_id', how='left')\n",
    "\n",
    "# Időkülönbség számítása az első eseményhez képest\n",
    "df_trial_szuk['time_diff'] = df_trial_szuk['event_time_new'] - df_trial_szuk['first_trial_time']\n",
    "\n",
    "# Szűrés a feltételek alapján\n",
    "df_trial_szuk = df_trial_szuk[(df_trial_szuk['time_diff'] <= timedelta(days=7)) | \n",
    "    (df_trial_szuk['event'] == 'trial_converted_event')].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kinduló adatvbázis: user-re grupba, de nem funkció, hanem esemény, és csak a szukitet adatokat (trial strat + 7 nap)\n",
    "# Számoljuk meg az egyes 'distinct_id' és 'event_name' kombinációkat\n",
    "df_trial_esem_szuk = df_trial_szuk.groupby(['distinct_id', 'event_name'])['event_name'].count().reset_index(name='count')\n",
    "\n",
    "# Pivotálás az események összesítéséhez 'distinct_id' szintjén\n",
    "df_pivot_esem_szuk = df_trial_esem_szuk.pivot(index='distinct_id', columns='event_name', values='count').fillna(0).astype(int)\n",
    "\n",
    "# Az eredeti 'distinct_id'-nél állandó user tulajdonsagok megőrzése - ez már fent ekészlt  afunkcióra pivotáláskor\n",
    "\n",
    "# Összefűzzük a pivotált DataFrame-et és a tulajdonsagokat\n",
    "df_alap_esem_szuk = pd.concat([df_constants, df_pivot_esem_szuk], axis=1).reset_index()\n",
    "\n",
    "# A 'distinct_id' visszaállítása oszlopként\n",
    "df_alap_esem_szuk = df_alap_esem_szuk.reset_index()\n",
    "df_alap_esem_szuk['distinct_id']  = df_alap_esem_szuk['index']\n",
    "df_alap_esem_szuk = df_alap_esem_szuk.drop(columns=['level_0', 'index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OSZTALYOZAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gradient boosting alap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN kezelése az os versionban\n",
    "df_alap_esem_szuk['os_version'] = df_alap_esem_szuk['os_version'].fillna('Unknown')\n",
    "\n",
    "# szukseges atkodolas\n",
    "le_country = LabelEncoder()\n",
    "df_alap_esem_szuk['country_code_encoded'] = le_country.fit_transform(df_alap_esem_szuk['country_code'])\n",
    "\n",
    "le_os_version = LabelEncoder()\n",
    "df_alap_esem_szuk['os_version_encoded'] = le_os_version.fit_transform(df_alap_esem_szuk['os_version'])\n",
    "\n",
    "\n",
    "# Az input változók kiválasztása\n",
    "Xs2 = df_alap_esem_szuk.drop(columns=['distinct_id', 'converted', 'country_code','os_version',  \n",
    "    'cancellation_event', 'expiration_event', 'initial_purchase_event' , 'product_change_event', 'renewal_event',\n",
    "    'trial_cancelled_event' , 'trial_converted_event', 'trial_started_event' , 'uncancellation_event', 'subscriptionStatus_didChange'])  #itt meg a vconvertalasi esemenyeket is ki kell venni\n",
    "ys2 = df_alap_esem_szuk['converted']\n",
    "\n",
    "# Adatok felosztása\n",
    "\n",
    "X_train_s2, X_test_s2, y_train_s2, y_test_s2 = train_test_split(Xs2, ys2, test_size=0.3, random_state=42)\n",
    "\n",
    "#tanitas\n",
    "gb_model_s3 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model_s3.fit(X_train_s2, y_train_s2)\n",
    "\n",
    "# kiértékelés\n",
    "y_pred_s3 = gb_model_s3.predict(X_test_s2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_s2, y_pred_s3))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_s2, y_pred_s3))\n",
    "\n",
    "y_pred_proba_s3 = gb_model_s3.predict_proba(X_test_s2)[:, 1]\n",
    "auroc_s3 = roc_auc_score(y_test_s2, y_pred_proba_s3)\n",
    "print(f\"AUROC: {auroc_s3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### False Negativok aránya javitésa sőlyozássaé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting létrehozása osztálysúlyokkal\n",
    "gb_model_s3FN = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit minták egyedi súlyozásával\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Példák súlyainak számítása az y_train osztályainak figyelembevételével\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_s2)\n",
    "\n",
    "# Modell betanítása\n",
    "gb_model_s3FN.fit(X_train_s2, y_train_s2, sample_weight=sample_weights)\n",
    "\n",
    "# FN Modell kiértékelése\n",
    "y_pred_s3FN = gb_model_s3FN.predict(X_test_s2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_s2, y_pred_s3FN))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_s2, y_pred_s3FN))\n",
    "\n",
    "y_pred_proba_s3FN = gb_model_s3FN.predict_proba(X_test_s2)[:, 1]\n",
    "auroc_s3FN = roc_auc_score(y_test_s2, y_pred_proba_s3FN)\n",
    "print(f\"AUROC: {auroc_s3FN:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE + FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE alkalmazása a train adatokra\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled_s, y_train_resampled_s = smote.fit_resample(X_train_s2, y_train_s2)\n",
    "\n",
    "# sample weights ujraszamolasa\n",
    "sample_weights3_s = compute_sample_weight(class_weight='balanced', y=y_train_resampled_s)\n",
    "\n",
    "\n",
    "# model betanítása\n",
    "gb_model_komb_s = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model_komb_s.fit(X_train_resampled_s, y_train_resampled_s, sample_weight=sample_weights3_s)\n",
    "\n",
    "# kiértékelés\n",
    "y_pred_komb_s = gb_model_komb_s.predict(X_test_s2)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_s2, y_pred_komb_s))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_s2, y_pred_komb_s))\n",
    "\n",
    "y_pred_proba_komb_s = gb_model_komb_s.predict_proba(X_test_s2)[:, 1]\n",
    "auroc_komb_s = roc_auc_score(y_test_s2, y_pred_proba_komb_s)\n",
    "print(f\"AUROC: {auroc_komb_s:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### legjobb a sulyozassal készült model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - s FN\n",
    "importances_s3FN = gb_model_s3FN.feature_importances_\n",
    "feature_names = Xs2.columns\n",
    "importance_dfs3FN = pd.DataFrame({'Feature': feature_names, 'Importance': importances_s3FN}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "importance_dfs3FN.to_excel('f_imp_s3FN.xlsx')\n",
    "\n",
    "# Feature importance s kombi\n",
    "importances_komb_s = gb_model_komb_s.feature_importances_\n",
    "feature_names = Xs2.columns\n",
    "importance_df_komb_s = pd.DataFrame({'Feature': feature_names, 'Importance': importances_komb_s}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "importance_df_komb_s.to_excel('f_imp_komb_s.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### legjobb model - sfn valószinúségei hozzáfúzése a userekhez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hozzáfúzzük aze redeti  df-hez uj oszlpként a legjobb modellel kiszámitott 1 osztályba esési valósziműséget\n",
    "df_alap_esem_szuk2 = df_alap_esem_szuk\n",
    "\n",
    "# Az oszlopokat kiválasztjuk a teljes adatbázisból (df_alap_esem), amit a modell megért.\n",
    "X_full = df_alap_esem_szuk2[X_train_s2.columns]  # Biztosítsuk, hogy az X_full azonos struktúrájú legyen, mint a tanítókészlet.\n",
    "\n",
    "# 1-es osztály valószínűség kiszámítása a teljes adatbázisra.\n",
    "df_alap_esem_szuk2['class_1_probability'] = gb_model_s3FN.predict_proba(X_full)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cel_szuk = df_alap_esem_szuk2.merge(df_alap[['distinct_id', 'Cluster']], how='left', on='distinct_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alap_esem_szuk2.to_excel('df_alap_esem_szuk.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### egyelőre vége :-) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
